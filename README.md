# TextSimplifier​

Проект для обучения и использования модели русскоязычной сверточной генерации аннотаций (суммаризации) текста на основе cointegrated/rut5-base.​  
Пайплайн включает подготовку датасета Gazeta, обучение модели, подбор гиперпараметров, вычисление метрик и интерактивный пример инференса.​

## Основные возможности​

- Подготовка данных на базе датасета IlyaGusev/gazeta с фильтрацией пар исходный текст → краткое резюме.​
- Обучение модели ruT5 на подготовленных данных с сохранением весов по эпохам в директорию models/.​
- Подбор гиперпараметров (learning rate, batch size) с помощью Optuna на подвыборке обучающих данных.​
- Расчёт метрик ROUGE на тестовой выборке для оценки качества суммаризации.​
- Интерактивный CLI‑скрипт для генерации краткого содержания введённого пользователем текста.​
- Вспомогательные утилиты: установка сидов, фильтрация пар, простая лемматизация и работа с n‑граммами.​
- Скрипт проверки доступности и конфигурации GPU (CUDA).​

## Стек и зависимости​

Проект реализован на Python с использованием PyTorch и библиотеки transformers для работы с моделью Seq2Seq.​  
Для работы с данными используются datasets, pandas и tqdm, для метрик — rouge-score, а для подбора гиперпараметров — optuna.​

Основные зависимости указаны в requirements.txt:​

- torch>=2.0
- transformers>=4.35
- datasets>=2.14
- rouge-score>=0.1, sacrebleu>=2.3
- nltk>=3.8, pymorphy2>=0.9
- pandas>=2.0, tqdm>=4.66
- optuna>=3.0, scikit-learn>=1.3
- python-telegram-bot>=20.0 и другие служебные библиотеки.

## Структура проекта​

TextSimplifier/

├─ config.py # Пути, гиперпараметры и базовое имя модели ruT5

├─ data_prepare_main.py # Загрузка/очистка/сплит датасета и сохранение в JSONL

├─ fit_model.py # Обучение модели на подготовленных данных

├─ ru_t5_optimize.py # Подбор гиперпараметров с Optuna

├─ calculate_metrics.py # Расчёт метрик ROUGE на тесте

├─ inference_example.py # Интерактивный пример инференса (CLI)

├─ utils.py # Утилиты: сид, фильтрация пар и др.

├─ test_gpu.py # Проверка доступности и работы CUDA

└─ requirements.txt # Список зависимостей

## Установка​

1.  Клонируйте репозиторий и перейдите в папку проекта.​
2.  Установите зависимости:​ pip install -r requirements.txt
3.  При необходимости настройте GPU‑окружение и проверьте скриптом:​ python test_gpu.py

## Конфигурация проекта​

Все основные пути и гиперпараметры задаются в config.py.​  
Здесь задаются директории data/raw, data/processed, models, имя базовой модели MODEL_NAME = "cointegrated/rut5-base", длины источника и цели, размер батча, число эпох, learning rate и фиксированный сид.​

Переменные окружения в config.py включают выбор GPU через CUDA_VISIBLE_DEVICES и отключение параллелизма токенизатора для стабильного логирования.​  
При первом запуске автоматически создаётся папка models/ для сохранения обученных весов.​

## Подготовка данных​

Скрипт data_prepare_main.py загружает датасет IlyaGusev/gazeta через datasets.load_dataset, берёт поля text и summary и складывает их в DataFrame с колонками src и tgt.​  
Далее данные очищаются, фильтруются и сохраняются в формат JSONL в директорию data/processed/&lt;имя_датасета&gt;.​

Фильтрация пар выполняется функцией filter_pair из utils.py, которая отбрасывает слишком короткие примеры, случаи, когда таргет почти совпадает с исходником, а также примеры, где длина таргета сопоставима или больше длины источника.​  
Готовый датасет случайно делится на сплиты train/val/test в пропорции 80/10/10 и сохраняется в файлы train.jsonl, val.jsonl, test.jsonl.​

Запуск подготовки данных:​ python data_prepare_main.py

## Обучение модели​

Скрипт fit_model.py загружает базовый токенизатор и модель AutoModelForSeq2SeqLM по имени MODEL_NAME и переносит её на доступное устройство (cuda или cpu).​  
Для воспроизводимости устанавливается сид через функцию set_seed из utils.py.​

Данные для обучения собираются из всех файлов \*/train.jsonl внутри data/processed, после чего формируется Dataset и DataLoader с собственной функцией collate_fn.​  
В collate_fn к текстам‑источникам добавляется префикс summarize:, выполняется токенизация с усечением/паддингом и формируются тензоры input_ids, attention_mask и labels с маскировкой паддинга значением −100−100.​

Оптимизация выполняется с помощью AdamW и линейного schedulerа get_linear_schedule_with_warmup, предусмотрена градиентная аккумуляция и обрезка градиентов по норме. \[attached_file:3\] После каждой эпохи веса модели сохраняются в models/rut5_finetuned_epoch{N}.pt\`, а в консоль выводится средний loss.​

Запуск обучения:​ python fit_model.py

## Подбор гиперпараметров (Optuna)​

Скрипт ru_t5_optimize.py использует Optuna для минимизации функции потерь train_one_epoch, импортируемой из fit_model.py.​  
Внутри objective подбираются learning rate (в диапазоне от 1⋅10−51⋅10−5 до 1⋅10−41⋅10−4 в лог‑шкале) и размер батча из набора \[4,8,16\]\[4,8,16\].​

Для ускорения эксперимента берётся подвыборка по 500 примеров из каждого файла train.jsonl и собирается единый DataLoader с той же collate_fn.​  
На каждой пробе создаётся новая модель ruT5, запускается одна эпоха обучения и возвращается полученный loss, который Optuna минимизирует в течение указанного числа трейалов.​

Запуск подбора гиперпараметров:​ python ru_t5_optimize.py

## Оценка качества (ROUGE)​

Скрипт calculate_metrics.py вычисляет метрики ROUGE‑1, ROUGE‑2 и ROUGE‑L для предсказаний модели на тестовых примерах.​  
Для этого используется rouge_scorer.RougeScorer с лемматизацией и стандартный пайплайн transformers.pipeline("summarization").​

Скрипт считывает тестовый файл data/processed/gazeta/test.jsonl, для каждого src генерирует аннотацию и затем считает средние значения ROUGE по всему набору.​  
Полученный словарь с метриками выводится в консоль, что позволяет сравнивать разные наборы гиперпараметров и версий модели.​

Пример запуска:​ python calculate_metrics.py

## Инференс и пример использования​

Скрипт inference_example.py демонстрирует загрузку обученной модели из файла models/rut5_finetuned_epoch3.pt и интерактивное общение с пользователем через консоль.​  
Скрипт автоматически выбирает устройство (cuda, если доступна, иначе cpu) и использует тот же токенизатор cointegrated/rut5-base.​

Функция generate_t5(text, max_len=128) добавляет к входному тексту префикс summarize:, токенизирует его и генерирует резюме с помощью model.generate с beam search, ограничениями по длине и штрафом за повторения.​

При запуске как \__main__ скрипт в цикле запрашивает у пользователя текст и печатает сгенерированную краткую аннотацию до ввода команды выхода.​

Запуск примера инференса:​ python inference_example.py

## Вспомогательные утилиты​

Файл utils.py содержит функции для установки случайных сидов во всех используемых библиотеках (random, numpy, torch, CUDA), что важно для воспроизводимости экспериментов.​  
Также реализованы простые токенизация и лемматизация русскоязычного текста на основе pymorphy2, функции для вычисления косинусного сходства векторов и перекрытия символьных n‑грамм.​

Функция filter_pair используется на этапе подготовки данных для отбрасывания неинформативных или некорректных пар исходный текст / таргет и учитывает длину в токенах, вложенность таргета и отношение длин.
